{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Assignment\n",
    "\n",
    "After you have tested some classifiers/regressors on the wines dataset (homework 1), you need to improve your models. Your tasks for this homework are:\n",
    "\n",
    "    Use gridsearch with cross validation to improve one of your previously submitted models (Homework 1) for each problem: regression, binary classification and multiclass classification. Select your parameters and scan their possible values properly (hint: use numpy). Report the test set score, best parameters, best cross-validation score and best estimator. What is the improvement with respect to the past results?\n",
    "\n",
    "    For your binary classification problem, use gridsearch and cross-validation with SVC. What is the best kernel? Print the heat map of mean cross-validation score (hint: use the mglearn library), the ROC curve and report the AUC value.\n",
    "\n",
    "    For your binary classification model, repeat the gridsearch with cross validation step using AUC as the evaluation metric. Do the result improves? Elaborate.\n",
    "\n",
    "You can work in small groups but you should submit your notebook individually. Deadline 15 of November at midnight.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# Download red wine data to Pandas dataframe\n",
    "redWine = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\",delimiter=\";\")\n",
    "# Check if there are some null values\n",
    "print(\"Are there are null values in redWine data frame? \"+ str(redWine.isnull().values.any()))\n",
    "print(\"redWine shape is:\")\n",
    "print(redWine.shape)\n",
    "# Add 1 as red wine type\n",
    "redWine[\"type\"] = 1\n",
    "display(redWine.head())\n",
    "\n",
    "# Download white wine data to Pandas dataframe\n",
    "whiteWine = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv\",delimiter=\";\")\n",
    "# Check if there are some null values\n",
    "print(\"Are there are null values in whiteWine data frame? \"+ str(whiteWine.isnull().values.any()))\n",
    "print(\"whiteWine shape is:\")\n",
    "print(whiteWine.shape)\n",
    "# Add 0 as white wine type\n",
    "whiteWine[\"type\"] = 0\n",
    "# Merge redWine and whiteWine dataframe \n",
    "wine = pd.concat([redWine, whiteWine],sort=False)\n",
    "print(\"the merged wine dataframe shape is:\")\n",
    "print(wine.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "# Use alcohol level as label and convert dataframe to numpy array\n",
    "X = wine.drop(['alcohol'], axis=1).values\n",
    "y = wine['alcohol'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "param_grid ={'alpha':[0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100],\n",
    "            'tol':[0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "GCV = GridSearchCV(Ridge(random_state=30), param_grid, scoring='r2',n_jobs =-1,cv=5, return_train_score=True)\n",
    "GCV.fit(X_train, y_train)\n",
    "print(\"The best parameters are: \"+ str(GCV.best_params_))\n",
    "print(\"The best score is: \"+str(GCV.best_score_))\n",
    "print(\"Mean train scores are: \"+ str(GCV.cv_results_['mean_train_score']))\n",
    "print(\"Mean test scores are: \"+ str(GCV.cv_results_['mean_test_score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use binary classifier to predict wine type, namely white wine or red wine\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "X = wine.drop(['type'], axis=1).values\n",
    "y = wine['type'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "param_grid ={'n_neighbors':[1, 2, 3, 4, 5, 6, 7, 8],\n",
    "            'p':[1,2],\n",
    "            'leaf_size':[20, 25, 30, 35, 40, 45]}\n",
    "GCV = GridSearchCV(KNeighborsClassifier(), param_grid, scoring='accuracy',n_jobs =-1,cv=5)\n",
    "GCV.fit(X_train, y_train)\n",
    "print(\"The best parameters are: \"+ str(GCV.best_params_))\n",
    "print(\"The best accuracy is: \"+str(GCV.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use multiclass classifier RandomForestClassifier to predict wine quality\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "X = wine.drop(['quality'], axis=1).values\n",
    "y = wine['quality'].values\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "display(dict(zip(unique, counts)))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "param_grid ={'n_estimators':[10, 50, 100, 150, 200, 250],\n",
    "             'min_samples_split':[2, 3, 4, 5, 6],\n",
    "             'min_samples_leaf':[1, 2, 3, 4, 5]}\n",
    "\n",
    "GCV = GridSearchCV(RandomForestClassifier(n_jobs =-1), param_grid, n_jobs =-1,cv=5)\n",
    "GCV.fit(X_train, y_train)\n",
    "print(\"The best parameters are: \"+ str(GCV.best_params_))\n",
    "print(\"The best accuracy is: \"+str(GCV.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use SVC classifier to predict wine type\n",
    "from sklearn.svm import SVC\n",
    "X = wine.drop(['type'], axis=1).values\n",
    "y = wine['type'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "param_grid = [{'kernel': ['rbf'],\n",
    "               'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "               'gamma': [0.001, 0.01, 0.1, 1, 10, 100]},\n",
    "              {'kernel': ['linear'],\n",
    "               'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "               'gamma': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "              ]\n",
    "\n",
    "GCV = GridSearchCV(SVC(), param_grid, n_jobs =-1,cv=5)\n",
    "GCV.fit(X_train, y_train)\n",
    "print(\"The best parameters are: \"+ str(GCV.best_params_))\n",
    "print(\"The best accuracy is: \"+str(GCV.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The best estimator is: {}\".format(GCV.best_estimator_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also tried other kernels, for example ‘poly’, ‘sigmoid’, ‘precomputed’. It took too much time, so we only compared 'rbf' and 'linear'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run SVC with linear kernel again to get mean_test_score\n",
    "param_grid = [{'kernel': ['linear'],\n",
    "               'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "               'gamma': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "              ]\n",
    "GCV = GridSearchCV(SVC(), param_grid, n_jobs =-1,cv=5)\n",
    "GCV.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = np.array(GCV.cv_results_['mean_test_score'].reshape(6, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mglearn\n",
    "# plot the mean cross-validation scores\n",
    "mglearn.tools.heatmap(scores, xlabel='gamma', xticklabels=param_grid[0]['gamma'],\n",
    "ylabel='C', yticklabels=param_grid[0]['C'], cmap=\"viridis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "svc = SVC(C=10, gamma=0.001, kernel='linear').fit(X_train, y_train)\n",
    "def plot_roc_curve():\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, svc.decision_function(X_test))\n",
    "    close_zero = np.argmin(np.abs(thresholds))\n",
    "    plt.plot(fpr, tpr, label=\"ROC Curve SVC\")\n",
    "    plt.xlabel(\"FPR\")\n",
    "    plt.ylabel(\"TPR (recall)\")\n",
    "    plt.plot(fpr[close_zero], tpr[close_zero], 'o', \n",
    "    label=\"threshold zero SVC\", fillstyle=\"none\", c='k', mew=2)\n",
    "    plt.legend(loc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " plot_roc_curve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "svc_auc = roc_auc_score(y_test, svc.decision_function(X_test))\n",
    "print(\"AUC for SVC: {:.3f}\".format(svc_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using AUC scoring instead:\n",
    "grid = GridSearchCV(SVC(), param_grid, scoring=\"roc_auc\")\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"\\nGrid-Search with AUC\")\n",
    "print(\"Best parameters:\", grid.best_params_)\n",
    "print(\"Best cross-validation score (AUC): {:.3f}\".format(grid.best_score_))\n",
    "print(\"Test set AUC: {:.3f}\".format(\n",
    "roc_auc_score(y_test, grid.decision_function(X_test))))\n",
    "print(\"Test set accuracy: {:.3f}\".format(grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We print how many wines are white and how many wines are red.\n",
    "# 1-red wine; 0-white wine.\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "display(dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With AUC as the scoring metric, the result(0.992) is better than the previous one(0.989).\n",
    "From the printed numbers of red and white wines, we see this dataset is not balanced. \n",
    "AUC is a better metric for imbalanced classification problems than accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
